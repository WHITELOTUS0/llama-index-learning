{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaIndex Introduction: Precision and Simplicity in Information Retrieval"
      ],
      "metadata": {
        "id": "nZwWUvNT_nL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Connectors\n",
        "The effectiveness of a RAG-based application is significantly enhanced by accessing a vector store that compiles information from various sources. However, managing data in diverse formats can be challenging.\n",
        "\n",
        "Data connectors, also called `Readers`, are essential in LlamaIndex. Readers are responsible for parsing and converting the data into a simplified Document representation consisting of text and basic metadata. Data connectors are designed to streamline the data ingestion process, automate the task of fetching data from various sources (like APIs, PDFs, and SQL databases), and format it.\n",
        "\n",
        "`LlamaHub` is an open-source project that hosts data connectors. `LlamaHub` repository offers data connectors for ingesting all possible data formats into the LLM.\n",
        "\n",
        "You can check out the LlamaHub repository and test some of the loaders here. You can explore various integrations and data sources with the embedded link below. These implementations make the preprocessing step as simple as executing a function. Take the Wikipedia integration, for instance."
      ],
      "metadata": {
        "id": "qZdam3bwFJ3P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA0-bfTU_mM7",
        "outputId": "a0387fa9-bda7-41cb-94b0-adb22f221fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/618.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m614.4/618.7 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.7/618.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.5/252.5 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.6/77.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.0/248.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.3/144.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "%pip install -q llama-index openai cohere deeplake==3.9.27 wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable Logging\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "#You can set the logging level to DEBUG for more verbose output,\n",
        "# or use level=logging.INFO for less detailed information.\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
        "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
      ],
      "metadata": {
        "id": "Wrp6W-l4E8UX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have also added a logging mechanism to the code. Logging in LlamaIndex is a way to monitor the operations and events that occur during the execution of your application. Logging helps develop and debug the process and understand the details of what the application is doing. In a production environment, you can configure the logging module to output log messages to a file or a logging service."
      ],
      "metadata": {
        "id": "Gwf1hLjOGQMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the `download_loader` method to access integrations from LlamaHub and activate them by passing the integration name to the class. In our sample code, the `WikipediaReader` class takes in several page titles and returns the text contained within them as `Document` objects."
      ],
      "metadata": {
        "id": "QShn_RRvGYOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import download_loader\n",
        "\n",
        "WikipediaReader = download_loader(\"WikipediaReader\")\n",
        "\n",
        "loader = WikipediaReader()\n",
        "\n",
        "documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n",
        "print(len(documents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLncsdVpGmqe",
        "outputId": "03aa9079-3158-4843-d04f-9abd68ddcba7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-805d263d7971>:3: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
            "  WikipediaReader = download_loader(\"WikipediaReader\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This retrieved information can be stored and utilized to enhance the knowledge base of our chatbot."
      ],
      "metadata": {
        "id": "zINA3GXmHEmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nodes\n",
        "In LlamaIndex, once data is ingested as documents, it passes through a processing structure that transforms these documents into `Node` objects. `Nodes` are smaller, more granular data units created from the original documents. Besides their primary content, these nodes also contain metadata and contextual information.\n",
        "\n",
        "LlamaIndex features a `NodeParser` class designed to convert the content of documents into structured nodes automatically. The `SimpleNodeParser` converts a list of document objects into nodes."
      ],
      "metadata": {
        "id": "DvGA3Zq0HKaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "\n",
        "# Assuming documents have already been loaded\n",
        "\n",
        "# Initialize the parser\n",
        "parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=20)\n",
        "\n",
        "# Parse documents into nodes\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "print(len(nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA4C3wbtHGPc",
        "outputId": "20b62b82-e938-4458-ea33-c93b2c56875d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above splits the two retrieved documents from the Wikipedia page into 58 smaller chunks with slight overlap."
      ],
      "metadata": {
        "id": "OPYmxUCOHlhk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indices\n",
        "At the heart of LlamaIndex is the capability to index and search various data formats like documents, PDFs, and database queries. Indexing is an initial step for storing information in a database; it essentially transforms the unstructured data into embeddings that capture semantic meaning and optimize the data format so it can be easily accessed and queried.\n",
        "\n",
        "LlamaIndex has a variety of index types, each fulfills a specific role. We have highlighted some of the popular index types in the following subsections."
      ],
      "metadata": {
        "id": "4P5vyRBzHwVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary Index\n",
        "The [Summary Index](https://docs.llamaindex.ai/en/stable/examples/index_structs/doc_summary/DocSummary/) extracts a summary from each document and stores it with all the nodes in that document. Since it’s not always easy to match small node embeddings with a query, sometimes having a document summary helps."
      ],
      "metadata": {
        "id": "TYoidM73H0-4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Store Index\n",
        "The Vector Store Index generates embeddings during index construction to identify the top-k most similar nodes in response to a query.\n",
        "\n",
        "It’s suitable for small-scale applications and easily scalable to accommodate larger datasets using high-performance vector databases."
      ],
      "metadata": {
        "id": "GerPt25XIGeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The crawled Wikipedia documents can be stored in a Deep Lake vector store, and an index object can be created based on its data. We can create the dataset in Activeloop and append documents to it by employing the `DeepLakeVectorStore` class. First, we need to set the Activeloop and OpenAI API keys in the environment using the following code."
      ],
      "metadata": {
        "id": "sTku6IAZIkQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key from Colab's userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Get the activeloop token from Colab's userdata\n",
        "activeloop_token = userdata.get('ACTIVELOOP_TOKEN')\n",
        "\n",
        "# Set it as an environment variable\n",
        "os.environ[\"ACTIVELOOP_TOKEN\"] = activeloop_token\n",
        "\n",
        "activeloop_org_id = userdata.get('ACTIVELOOP_ORG_ID')\n",
        "\n",
        "os.environ[\"ACTIVELOOP_ORG_ID\"] = activeloop_org_id"
      ],
      "metadata": {
        "id": "ssj1AuKQI78F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To connect to the platform, use the `DeepLakeVectorStore` class and provide the `dataset path` as an argument.\n",
        "Running the following code will create an empty dataset."
      ],
      "metadata": {
        "id": "TcTQ9V9fJjRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-vector-stores-deeplake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VY6h9q-K08F",
        "outputId": "d9095a70-7f25-48df-d361-7a0e54043aad"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-vector-stores-deeplake\n",
            "  Downloading llama_index_vector_stores_deeplake-0.3.2-py3-none-any.whl.metadata (730 bytes)\n",
            "Requirement already satisfied: deeplake>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-deeplake) (3.9.27)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-deeplake) (0.12.12)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.11/dist-packages (from llama-index-vector-stores-deeplake) (2.10.1)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (1.26.4)\n",
            "Requirement already satisfied: pillow~=10.4.0 in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (10.4.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (1.36.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (8.1.8)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (0.3.3)\n",
            "Requirement already satisfied: humbug>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (0.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (4.67.1)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (4.3.3)\n",
            "Requirement already satisfied: aioboto3>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (13.4.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (1.6.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (2.10.5)\n",
            "Requirement already satisfied: libdeeplake==0.0.147 in /usr/local/lib/python3.11/dist-packages (from deeplake>=3.9.12->llama-index-vector-stores-deeplake) (0.0.147)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from libdeeplake==0.0.147->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (0.3.9)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (3.11.11)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.2.15)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.0.8)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (0.28.1)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (3.4.2)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (9.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.17.0)\n",
            "Requirement already satisfied: aiobotocore==2.18.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore[boto3]==2.18.0->aioboto3>=10.4.0->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (2.18.0)\n",
            "Requirement already satisfied: aiofiles>=23.2.1 in /usr/local/lib/python3.11/dist-packages (from aioboto3>=10.4.0->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (24.1.0)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3>=10.4.0->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (0.12.0)\n",
            "Requirement already satisfied: botocore<1.36.2,>=1.36.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3>=10.4.0->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (1.36.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3>=10.4.0->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (2.8.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3>=10.4.0->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (1.0.1)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3>=10.4.0->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (6.1.0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3>=10.4.0->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (2.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.18.3)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (0.11.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (3.1.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (3.26.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (0.14.0)\n",
            "Requirement already satisfied: ppft>=1.7.6.9 in /usr/local/lib/python3.11/dist-packages (from pathos->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (1.7.6.9)\n",
            "Requirement already satisfied: pox>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from pathos->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (0.3.5)\n",
            "Requirement already satisfied: multiprocess>=0.70.17 in /usr/local/lib/python3.11/dist-packages (from pathos->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (0.70.17)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (24.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-vector-stores-deeplake) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->aiobotocore==2.18.0->aiobotocore[boto3]==2.18.0->aioboto3>=10.4.0->deeplake>=3.9.12->llama-index-vector-stores-deeplake) (1.17.0)\n",
            "Downloading llama_index_vector_stores_deeplake-0.3.2-py3-none-any.whl (7.1 kB)\n",
            "Installing collected packages: llama-index-vector-stores-deeplake\n",
            "Successfully installed llama-index-vector-stores-deeplake-0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores.deeplake import DeepLakeVectorStore\n",
        "\n",
        "my_activeloop_org_id = activeloop_org_id\n",
        "my_activeloop_dataset_name = \"LlamaIndex_intro\"\n",
        "dataset_path = f\"hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}\"\n",
        "\n",
        "# Create an index over the documnts\n",
        "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSdsd7ILJt9A",
        "outputId": "63b9b10c-c218-415a-931a-5ba3bccb296f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Deep Lake dataset has been successfully created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need to create a storage context using the `StorageContext` class and the Deep Lake dataset as the source. Pass this storage to a `VectorStoreIndex` class to create the index (generate embeddings) and store the results on the defined dataset."
      ],
      "metadata": {
        "id": "GvFK7g0PNnqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.storage.storage_context import StorageContext\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGR8_2yANxXW",
        "outputId": "17dbcea8-d811-4aff-9997-07d5b47020a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading data to deeplake dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [00:00<00:00, 66.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset(path='hub://glorrysibomana758/LlamaIndex_intro', tensors=['text', 'metadata', 'embedding', 'id'])\n",
            "\n",
            "  tensor      htype      shape      dtype  compression\n",
            "  -------    -------    -------    -------  ------- \n",
            "   text       text      (31, 1)      str     None   \n",
            " metadata     json      (31, 1)      str     None   \n",
            " embedding  embedding  (31, 1536)  float32   None   \n",
            "    id        text      (31, 1)      str     None   \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The created database will be accessible in the future. The Deep Lake database efficiently stores and retrieves high-dimensional vectors."
      ],
      "metadata": {
        "id": "oOZFw59nOefm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Engines\n",
        "\n",
        "The next step is to leverage the generated indexes to query through the information. The Query Engine is a wrapper that combines a Retriever and a Response Synthesizer into a pipeline. The pipeline uses the query string to fetch nodes and then sends them to the LLM to generate a response. A query engine can be created by calling the `as_query_engine()` method on an already-created index.\n",
        "\n",
        "The code below uses the documents fetched from the Wikipedia page to construct a Vector Store Index using the `GPTVectorStoreIndex` class. The `.from_documents()` method simplifies building indexes on these processed documents. The created index can then be utilized to generate a `query_engine` object, allowing us to ask questions based on the documents using the `.query()` method."
      ],
      "metadata": {
        "id": "K2aZ_2kwPfcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import GPTVectorStoreIndex\n",
        "\n",
        "index = GPTVectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What does NLP stands for?\")\n",
        "print( response.response )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X8aMT2bQBXF",
        "outputId": "088e2384-d493-438f-a95f-ca2e52700ef8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLP stands for natural language processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The indexes can also function solely as retrievers for fetching documents relevant to a query. This capability enables the creation of a Custom Query Engine, offering more control over various aspects, such as the prompt or the output format."
      ],
      "metadata": {
        "id": "ikvsVNX2Sh1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Routers\n",
        "Routers play a role in determining the most appropriate retriever for extracting context from the knowledge base. The routing function selects the optimal query engine for each task, improving performance and accuracy.\n",
        "\n",
        "These functions are beneficial when dealing with multiple data sources, each holding unique information. Consider an application that employs a SQL database and a Vector Store as its knowledge base. In this setup, the router can determine which data source is most applicable to the given query."
      ],
      "metadata": {
        "id": "uBHIYVhJTNcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving and Loading Indexes Locally\n",
        "All the examples we explored involved storing indexes on cloud-based vector stores like Deep Lake. However, there are scenarios where saving the data on a disk might be necessary for rapid testing. The concept of storing refers to saving the index data, which includes the nodes and their associated embeddings, to disk. This is done using the `persist()` method from the ``storage_context object related to the index."
      ],
      "metadata": {
        "id": "L3Vfwu0qTl8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store index as vector embeddings on the disk\n",
        "index.storage_context.persist()\n",
        "# This saves the data in the 'storage' by default\n",
        "# to minimize repetitive processing"
      ],
      "metadata": {
        "id": "pRdz5c3LT97j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the index already exists in storage, you can load it directly instead of recreating it. We simply need to determine whether the index already exists on disk and proceed accordingly; here is how to do it:"
      ],
      "metadata": {
        "id": "i04qsmfAUjRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index Storage Checks\n",
        "import os.path\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        "    load_index_from_storage,\n",
        ")\n",
        "from llama_index.core import download_loader\n",
        "\n",
        "# Let's see if our index already exists in storage.\n",
        "if not os.path.exists(\"./storage\"):\n",
        "    # If not, we'll load the Wikipedia data and create a new index\n",
        "    WikipediaReader = download_loader(\"WikipediaReader\")\n",
        "    loader = WikipediaReader()\n",
        "    documents = loader.load_data(pages=['Natural Language Processing', 'Artificial Intelligence'])\n",
        "    index = VectorStoreIndex.from_documents(documents)\n",
        "    # Index storing\n",
        "    index.storage_context.persist()\n",
        "\n",
        "else:\n",
        "    # If the index already exists, we'll just load it:\n",
        "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "    index = load_index_from_storage(storage_context)"
      ],
      "metadata": {
        "id": "KX27od9QUlo6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the `os.path.exists(\"./storage\")` function is used to check if the 'storage' directory exists. If it does not exist, the Wikipedia data is loaded, and a new index is created."
      ],
      "metadata": {
        "id": "X_ckdhuaU4tv"
      }
    }
  ]
}